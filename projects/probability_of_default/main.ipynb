{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a2d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.core.config import initialize_daanish, load_project_config\n",
    "from utils.data_io import load_data\n",
    "from utils.core.feature_manager import FeatureManager\n",
    "from utils.eda.descriptive import DescriptiveAnalysis\n",
    "from utils.viz.display import DisplayUtils\n",
    "from utils.core.save_manager import SaveUtils\n",
    "from utils.core.format_utils import FormatUtils\n",
    "from utils.eda.statistical import StatisticalAnalysis\n",
    "from utils.eda.visualisation.general_viz import Visualisation\n",
    "from utils.eda.correlation import CorrelationAnalyzer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.preprocessing.missing_values import MissingValueHandler\n",
    "from utils.features.selector import FeatureSelector\n",
    "from utils.eda.outlier_detection import OutlierDetector\n",
    "from utils.preprocessing.outlier_treatment import OutlierHandler\n",
    "from utils.dimensionality.multicollinearity import MulticollinearityDetector\n",
    "from utils.dimensionality.pca_analyzer import PCAAnalyzer\n",
    "from utils.dimensionality.mca_analyzer import MCAAnalyzer\n",
    "from utils.dimensionality.famd_analyzer import FAMDAnalyzer\n",
    "from utils.eda.visualisation.dimensionality_viz import DimensionalityViz\n",
    "from utils.modelling.clustering.kmeans_cluster import KMeansClustering\n",
    "from utils.eda.visualisation.cluster_viz import ClusterVisualisation\n",
    "from utils.modelling.clustering.hierarchical_cluster import HierarchicalClustering\n",
    "from utils.modelling.clustering.dbscan_cluster import DBSCANClustering  \n",
    "from utils.modelling.clustering.kmodes_cluster import KModesClustering\n",
    "from utils.modelling.clustering.kprototypes_cluster import KPrototypesClustering\n",
    "from utils.modelling.clustering.hierarchical_gower_cluster import HierarchicalGowerClustering\n",
    "from utils.features.binning import Binner  \n",
    "from utils.importance.woe_encoder import WOEEncoder\n",
    "from utils.eda.visualisation.feature_importance_viz import FeatureImportanceVisualiser\n",
    "from utils.importance.iv_calculator import IVCalculator\n",
    "from utils.preprocessing.data_encoder import DataEncoder\n",
    "from utils.modelling.classification.logistic_regression import LogisticModel\n",
    "from utils.modelling.classification.random_forest import RandomForestModel\n",
    "from utils.modelling.classification.xgboost import XGBoostModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba7f734",
   "metadata": {},
   "source": [
    "#### Step 1: Project Initialization and Data Loading \n",
    "In this step, we:\n",
    "- Initialize the Daanish core setup\n",
    "- Access global and project-specific configuration values\n",
    "- Construct input and output paths based on project settings\n",
    "- Load the main dataset for modeling\n",
    "- Load the list of model features along with their attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4f423",
   "metadata": {},
   "source": [
    "- Initialize the Daanish core setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_config = initialize_daanish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25eef0a",
   "metadata": {},
   "source": [
    "- Access global and project-specific configuration values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e0409",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "project_root = os.getcwd()\n",
    "project_config = load_project_config(project_root)\n",
    "\n",
    "input_data_folder = project_config.get('paths', 'input_data_folder')\n",
    "output_data_folder = project_config.get('paths', 'output_data_folder')\n",
    "main_dataset = project_config.get('input_files', 'main_dataset')\n",
    "model_features = project_config.get('input_files', 'features_attributes')\n",
    "source_type = project_config.get('datasource_type', 'source_type')\n",
    "main_dataset_query = project_config.get('db_queries', 'main_dataset_query')\n",
    "model_features_query = project_config.get('db_queries', 'model_features_query')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ede6a",
   "metadata": {},
   "source": [
    "- Construct input and output paths based on project settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2531fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_path = os.path.join(project_root, input_data_folder)\n",
    "output_path = os.path.join(project_root, output_data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0678bdf3",
   "metadata": {},
   "source": [
    "- Load the main dataset for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5122cd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = load_data(\n",
    "    source_type=source_type,\n",
    "    input_path=os.path.join(input_path, main_dataset),\n",
    "    query=main_dataset_query,\n",
    "    global_config=global_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8665cc68",
   "metadata": {},
   "source": [
    "- Load the list of model features along with their attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_manager = FeatureManager(\n",
    "    source_type=source_type,\n",
    "    input_path=os.path.join(input_path, model_features),\n",
    "    global_config=global_config,\n",
    "    query=model_features_query\n",
    ")\n",
    "\n",
    "# Feature types\n",
    "nominal_features = feature_manager.get_nominal_features()\n",
    "ordinal_features = feature_manager.get_ordinal_features()\n",
    "numerical_features = feature_manager.get_numerical_features()\n",
    "target_variable = feature_manager.get_target_variable()\n",
    "all_features = feature_manager.get_all_features()\n",
    "\n",
    "# Missing value handling\n",
    "missing_value_strategies = feature_manager.get_missing_value_strategies()\n",
    "missing_fill_values = feature_manager.get_missing_fill_values()\n",
    "\n",
    "# Display names\n",
    "display_names = feature_manager.get_display_names()\n",
    "\n",
    "# Outlier handling configs\n",
    "outlier_strategies = feature_manager.get_outlier_detection_strategies()\n",
    "outlier_params = feature_manager.get_outlier_detection_params()\n",
    "outlier_imputation_methods = feature_manager.get_outlier_imputation_methods()\n",
    "outlier_imputation_values = feature_manager.get_outlier_imputation_values()\n",
    "outlier_config_bundle = feature_manager.get_outlier_config_bundle()\n",
    "\n",
    "# Features' binning config\n",
    "binning_config = feature_manager.get_binning_config_bundle()\n",
    "\n",
    "# print(\"Nominal Features:\", nominal_features)\n",
    "# print(\"Ordinal Features:\", ordinal_features)\n",
    "# print(\"Numerical Features:\", numerical_features)\n",
    "# print(\"Target Variable:\", target_variable)\n",
    "# print(\"All Features:\", all_features)\n",
    "# print(\"Missing Value Strategies:\", missing_value_strategies)\n",
    "# print(\"Missing Fill Values:\", missing_fill_values)\n",
    "# print(\"Display Names:\", display_names)\n",
    "# print(\"Binning Config:\", binning_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba1ccff",
   "metadata": {},
   "source": [
    "#### Step 2: Preliminary Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this step, we explore the raw dataset to understand its structure, identify potential issues (e.g., missing values, outliers, inconsistent types), and gain initial insights into data distributions. This provides the foundation for informed preprocessing and feature engineering decisions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the `DescriptiveAnalysis` class with our main dataset\n",
    "eda_desc = DescriptiveAnalysis(main_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ac0311",
   "metadata": {},
   "source": [
    "- 2.1 data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = eda_desc.get_data_samples(5)\n",
    "DisplayUtils.show_dataframe_notebook(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c06ae",
   "metadata": {},
   "source": [
    "- 2.2 dataset summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc298f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_summary = eda_desc.get_dataset_summary()\n",
    "DisplayUtils.show_summary_console(dataset_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc9f3e",
   "metadata": {},
   "source": [
    "- 2.3 Summary of Feature(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aac227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_feature_summary = eda_desc.get_feature_summary(\"loan_amnt\")\n",
    "All_features_summary = eda_desc.get_all_feature_summaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd07519",
   "metadata": {},
   "source": [
    "- Display Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DisplayUtils.print_feature_summary(\"loan_amnt\", single_feature_summary)\n",
    "DisplayUtils.print_high_level_summary(All_features_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28026c22",
   "metadata": {},
   "source": [
    "- Save High Level Descriptive Analysis Summary to CSV and Excel\n",
    "\n",
    "This cell formats the high-level feature summaries into a structured DataFrame and saves it as a CSV or Excel file. The formatting is handled by `FormatUtils`, which extracts selected statistics for each feature, and the output is saved using `SaveUtils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06bfa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the `SaveUtils` class\n",
    "save_utils = SaveUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90061da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the summary\n",
    "df_summary = FormatUtils.high_level_summary_to_dataframe(All_features_summary)\n",
    "\n",
    "# Save as a CSV file\n",
    "# save_utils.save_dataframe_to_csv(df_summary, os.path.join(output_path, \"descriptive_summary.csv\"), overwrite=True)\n",
    "\n",
    "# Save as an Excel file\n",
    "# save_utils.save_dataframe_to_excel(df_summary, os.path.join(output_path, \"descriptive_summary.xlsx\"), sheet_name='Descriptive Summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff07f4",
   "metadata": {},
   "source": [
    "- Save Detailed Descriptive Analysis Summary to JSON\n",
    "This cell saves the detailed descriptive analysis summary as a JSON file.  \n",
    "It is intended for use by applications or services that need to consume and display the analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd44192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as JSON file\n",
    "# save_utils.save_json(All_features_summary, os.path.join(output_path, \"descriptive_summary.json\"), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018aeb0e",
   "metadata": {},
   "source": [
    "### 🔍 2.4 Find the Best-Fit Probability Distribution for Selected Features\n",
    "This cell identifies the best-fit probability distribution for each feature in the given list.\n",
    "- **Method**: Defines the criterion for selecting the best fit. Options are:\n",
    "  - `'sumsquare_error'` *(default)*\n",
    "  - `'aic'`\n",
    "  - `'bic'`\n",
    "\n",
    "- **common_distributions (bool)**:  \n",
    "  - If `True`, only fits a curated list of commonly used distributions:  \n",
    "    `'norm'`, `'expon'`, `'lognorm'`, `'gamma'`, `'beta'`, `'weibull_min'`, `'chi2'`, `'pareto'`, `'uniform'`, `'t'`, `'gumbel_r'`, `'burr'`, `'invgauss'`, `'triang'`, `'laplace'`, `'logistic'`, `'genextreme'`, `'skewnorm'`, `'genpareto'`, `'burr12'`, `'fatiguelife'`, `'geninvgauss'`, `'halfnorm'`, `'exponpow'`\n",
    "\n",
    "  - If `False`, fits from an extended list of over 100 SciPy continuous distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe1ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the `StatisticalAnalysis` class\n",
    "eda_stat = StatisticalAnalysis(main_df)\n",
    "\n",
    "# Finding the best-fit probability distribution\n",
    "# distribution_results = eda_stat.fit_best_distribution(numerical_features, method='sumsquare_error', common_distributions=True, timeout=60)\n",
    "distribution_results = eda_stat.fit_best_distribution(['person_income'], method='sumsquare_error', common_distributions=True, timeout=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55226da8",
   "metadata": {},
   "source": [
    "- Plotting the best-fit distributions for a given feature(s) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the `Visualisation` class\n",
    "viz = Visualisation(main_df, display_names)\n",
    "\n",
    "# Plotting best-fit distributions\n",
    "# viz.plot_distributions(fitted_distributions = distribution_results,variables=numerical_features)\n",
    "# viz.plot_distributions(fitted_distributions = distribution_results,variables=['person_age', 'loan_amnt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06179db",
   "metadata": {},
   "source": [
    "- 📊Plot Histograms for Selected Features\n",
    "This cell visualizes the distribution of selected features using histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f150b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # viz.plot_histogram(variables=numerical_features, orientation=\"vertical\")\n",
    "# viz.plot_histogram(variables=['loan_amnt'], orientation=\"vertical\")\n",
    "\n",
    "    # viz.plot_histogram(variables=nominal_features, orientation=\"horizontal\")\n",
    "    # viz.plot_histogram(variables=['loan_intent'], orientation=\"horizontal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8026a06a",
   "metadata": {},
   "source": [
    "- 🔵 Scatter Plots for Relationship Analysis\n",
    "This cell provides visualizations to explore relationships between two numerical variables, with optional grouping and trendlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425e9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot with color based on `loan_grade`\n",
    "# viz.plot_scatter(x_var=\"person_income\", y_var=\"person_age\", hue_var=\"loan_status\", trendline=True)\n",
    "# viz.plot_scatter(x_var=\"person_income\", y_var=\"loan_amnt\", hue_var=\"loan_status\", trendline=True)\n",
    "# viz.plot_scatter(x_var=\"loan_int_rate\", y_var=\"loan_amnt\", hue_var=\"loan_status\", trendline=True)\n",
    "# viz.plot_scatter(x_var=\"loan_int_rate\", y_var=\"person_income\", hue_var=\"loan_status\", trendline=True)\n",
    "\n",
    "\n",
    "# Scatter plot with trendline\n",
    "# viz.plot_scatter(x_var=\"person_age\", y_var=\"person_income\", trendline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f8edd",
   "metadata": {},
   "source": [
    "- 🟩 Box Plots for Distribution Comparison\n",
    "\n",
    "This cell uses box plots to compare the distribution of a numerical variable across categories of another feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c7278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz.plot_boxplot(column='person_income', by='loan_intent')\n",
    "# viz.plot_boxplot(column='loan_amnt', by='person_home_ownership')\n",
    "# viz.plot_boxplot(column='loan_int_rate', by='cb_person_default_on_file')\n",
    "# viz.plot_boxplot(column='person_age', by='loan_intent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37239b8d",
   "metadata": {},
   "source": [
    "- 📊 Crosstab Analysis\n",
    "This section generates cross-tabulation (contingency) tables to explore the relationship between categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c922f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For two variables\n",
    "crosstab_result_1 = eda_stat.crosstab(\"loan_status\", \"person_home_ownership\", normalize=\"index\")\n",
    "crosstab_result_2 = eda_stat.crosstab(\"loan_status\", \"loan_intent\", normalize=\"index\")\n",
    "crosstab_result_3 = eda_stat.crosstab(\"loan_status\", \"cb_person_default_on_file\", normalize=\"index\")\n",
    "\n",
    "# For three variables\n",
    "crosstab_result_4 = eda_stat.crosstab_three_way(\"loan_status\", \"cb_person_default_on_file\", \"person_home_ownership\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display\n",
    "# crosstab_result_1\n",
    "# crosstab_result_2\n",
    "# crosstab_result_3\n",
    "crosstab_result_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42337ed6",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing\n",
    "\n",
    "- 3.1 Missing Value Handling  \n",
    "- 3.2 Outlier Handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa69e0",
   "metadata": {},
   "source": [
    "#### 🧼 3.1 Missing Value Handling Pipeline\n",
    "\n",
    "This pipeline addresses missing data in a modular and strategy-driven manner. It includes detection, filtering, and imputation using configurable rules per feature.\n",
    "\n",
    "---\n",
    "\n",
    "##### 🔍 Identify and Remove Features with Excessive Missingness\n",
    "\n",
    "A threshold (e.g., 30%) is used to detect features with too many missing values. These features are removed from the dataset to prevent model instability, unreliable imputations, or learning bias.\n",
    "\n",
    "---\n",
    "\n",
    "##### 🛠️ Impute Remaining Missing Values with Defined Strategies\n",
    "\n",
    "For the remaining features, missing values are imputed according to user-specified strategies. Each feature can have a tailored strategy from the options below:\n",
    "\n",
    "- `\"drop\"`: Remove rows with missing values in this feature.\n",
    "- `\"fill_mean\"`: Fill with the mean (numeric features only).\n",
    "- `\"fill_median\"`: Fill with the median (numeric features only).\n",
    "- `\"fill_mode\"`: Fill with the most frequent value.\n",
    "- `\"fill_value\"`: Fill using a custom value (requires `fill_values`).\n",
    "- `\"ffill\"`: Forward fill (carry last valid value forward).\n",
    "- `\"bfill\"`: Backward fill (use next valid value).\n",
    "- `\"fill_interpolate\"`: Linearly interpolate between valid values.\n",
    "- `\"none\"` or `\"keep\"`: Retain missing values (no action taken).\n",
    "\n",
    "The missing value handler processes these strategies on a per-feature basis and returns:\n",
    "\n",
    "- **`imputed_records`**: A subset of rows where imputation occurred, with an `affected_features` column.\n",
    "- **`imputed_dataset`**: The complete dataset after imputation and/or row removal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1fc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize handler and identify features with high missing rates\n",
    "dp = MissingValueHandler(main_df)\n",
    "features_with_high_missing = dp.features_with_many_missing(threshold=0.3)\n",
    "print(\"Features with high missing values: \", features_with_high_missing)\n",
    "\n",
    "# Step 2: Drop high-missing features and update the working dataset\n",
    "fs = FeatureSelector(main_df)\n",
    "updated_df = fs.drop_features(features_with_high_missing)\n",
    "\n",
    "# Step 3: Reinitialize missing value handler with cleaned dataset\n",
    "dp = MissingValueHandler(updated_df)\n",
    "\n",
    "# Step 4: Filter feature lists and strategies to match updated dataset\n",
    "all_features = [f for f in all_features if f not in features_with_high_missing]\n",
    "strategies = {k: v for k, v in missing_value_strategies.items() if k in all_features}\n",
    "fill_values = {k: v for k, v in missing_fill_values.items() if k in all_features}\n",
    "\n",
    "# Step 5: Apply missing value imputation strategies\n",
    "imputed_records, imputed_dataset = dp.handle(\n",
    "    all_features, strategies=strategies, fill_values=fill_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384ee793",
   "metadata": {},
   "source": [
    "#### 📉 3.2 Outlier Detection and Handling\n",
    "This stage identifies and flags outliers in numerical features using a variety of statistical and machine learning methods.\n",
    "Available techniques include:\n",
    "- IQR-based detection\n",
    "- Z-score filtering\n",
    "- Isolation Forest\n",
    "- Local Outlier Factor (LOF)\n",
    "- Distribution fitting (e.g., lognorm, gamma)\n",
    "- Custom user-defined bounds\n",
    "\n",
    "Once detection strategies are tested and validated, a unified pipeline is executed using detect_outliers_featurewise(), which applies the appropriate detection method per feature based on predefined configuration.\n",
    "Then, outliers will be treated based on defined imputation methods, such as replacing with mean, median, mode, or using a custom value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d7088",
   "metadata": {},
   "source": [
    "##### 3.2.1 Testing Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd1d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = OutlierDetector(imputed_dataset)\n",
    "# outlier_df = outliers.detect_outliers_distribution(distribution_results,confidence_interval=0.999)\n",
    "outlier_df = outliers.detect_outliers_iqr(['person_income'])\n",
    "# outlier_df = outliers.detect_outliers_isolation_forest(['person_income'])\n",
    "# outlier_df = outliers.detect_outliers_lof(['person_income'])\n",
    "# outlier_df = outliers.detect_outliers_zscore(['person_income'])\n",
    "# outlier_df = outliers.detect_custom_outliers(['person_age'],upper_bounds={'person_age': 100})\n",
    "# print(outlier_df) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f21d93a",
   "metadata": {},
   "source": [
    "##### 3.2.2 Executing a unified pipeline to identify outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bf6a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = OutlierDetector(imputed_dataset)\n",
    "outlier_df = outliers.detect_outliers_featurewise(\n",
    "    method_config=outlier_config_bundle,\n",
    "    distribution_results=distribution_results\n",
    ")\n",
    "# print(outlier_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735a333",
   "metadata": {},
   "source": [
    "##### 3.2.3 Executing imputation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f905be4c",
   "metadata": {},
   "source": [
    "Removes rows where the proportion of outlier features exceeds the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = OutlierHandler(imputed_dataset)\n",
    "row_wise_filtered_df = handler.filter_outlier_heavy_rows(outliers_df=outlier_df,threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dccece6",
   "metadata": {},
   "source": [
    "Apply imputation method per feature's outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d5029",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = OutlierHandler(row_wise_filtered_df)\n",
    "handled_records, cleaned_df = handler.handle_from_config(\n",
    "    outlier_config_bundle=outlier_config_bundle,\n",
    "    outliers_df=outlier_df\n",
    ")\n",
    "# print(handled_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a1e832",
   "metadata": {},
   "source": [
    "## Step 4: Full Exploratory Data Analysis (EDA)\n",
    "- Analyse feature correlations, multicollinearity, and interactions.\n",
    "- Use dimensionality-reduction techniques (e.g., PCA) or clustering to explore structure.\n",
    "- Assess relationships between features and the target variable (Default).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3486efdd",
   "metadata": {},
   "source": [
    "#### 🔗 4.1 Examine Variable Correlations\n",
    "\n",
    "This section calculates and displays correlations between different types of variables in the cleaned (`cleaned_df`) dataset.\n",
    "\n",
    "- **`num_method` (str)**: Defines the method for calculating correlation between numerical variables. Allowed values are:\n",
    "    - `'pearson'` *(default)*: Standard Pearson linear correlation coefficient.\n",
    "    - `'spearman'`: Spearman's rank correlation coefficient (for monotonic relationships).\n",
    "    - `'kendall'`: Kendall's tau correlation coefficient (for ordinal or non-normally distributed data).\n",
    "\n",
    "- **`cat_method` (str)**: Defines the method for calculating association between categorical variables. Allowed values are:\n",
    "    - `'cramers_v'` *(default)*: Cramer's V (measures association between nominal categorical variables).\n",
    "    - `'mutual_info'`: Mutual Information (measures the statistical dependence between two random variables).\n",
    "\n",
    "- **`cat_num_method` (str)**: Defines the method for calculating association between \n",
    "    categorical and numerical variables. Allowed values are:\n",
    "    - `'correlation_ratio'` *(default)*: Correlation Ratio (Eta squared, measures variance explained).\n",
    "    - `'f_test'`: F-statistic from ANOVA (assesses the difference in means across categories).\n",
    "    - `'mutual_info'`: Mutual Information (measures the statistical dependence).\n",
    "    - `'kruskal'`: Non-parametric alternative to ANOVA. Compares distributions of a continuous variable across categories. Good when your numerical variables are not normally distributed\n",
    "    - `'target_spearman'`: Replaces each category with the mean of the target variable (e.g. default rate). Then computes correlation with numerical features. Captures ordinal structure or monotonic trends across groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d951ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_analyzer = CorrelationAnalyzer(cleaned_df)\n",
    "corr_df,corr_matrix = corr_analyzer.correlation_matrix(num_method=\"spearman\", cat_method=\"cramers_v\",\n",
    "                                      cat_num_method=\"mutual_info\",return_matrix=True)\n",
    "\n",
    "Visualisation.plot_heatmap_matrix(corr_matrix, title=\"Correlation Matrix\")\n",
    "\n",
    "save_utils.save_dataframe_to_csv(corr_matrix, os.path.join(output_path, \"correlation_matrix.csv\"), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdde39a",
   "metadata": {},
   "source": [
    "#### 4.2 Multicollinearity Detection\n",
    "\n",
    "This section is identifying and resolving multicollinearity among numerical features in a dataset using:\n",
    "- **Variance Inflation Factor (VIF)**.\n",
    "- **Pairwise correlation analysis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb49cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = MulticollinearityDetector(cleaned_df, correlation_matrix=corr_matrix)\n",
    "\n",
    "# VIF values\n",
    "print(detector.compute_vif())\n",
    "\n",
    "# Correlated pairs\n",
    "print('High Correlation Pairs: ',detector.high_correlation_pairs())\n",
    "\n",
    "# Suggestions for dropping features with high correlation and multicollinearity\n",
    "print('Suggestion for Features to drop: ', detector.suggest_features_to_drop())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cc1239",
   "metadata": {},
   "source": [
    "#### 4.3 PCA and MCA Analysis\n",
    "In this section, we reduce the dimensionality of our dataset using Principal Component Analysis (PCA) for numeric features and Multiple Correspondence Analysis (MCA) for categorical features. These methods help uncover latent patterns, simplify complexity, and enhance visualization by projecting high-dimensional data into lower-dimensional spaces while preserving as much variability as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7609a42b",
   "metadata": {},
   "source": [
    "##### 4.3.1 PCA Analysis on Numerical Features\n",
    "\n",
    "In this step, we perform Principal Component Analysis (PCA) to explore the structure of the numerical feature space and understand the variance explained by the principal components.\n",
    "\n",
    "Steps:\n",
    "1. **Drop non-numerical features and the target variable** to ensure PCA only processes numerical data.\n",
    "2. **Fit PCA** on the cleaned numerical dataset.\n",
    "3. **Plot explained variance** to decide how many components capture most of the data’s variance.\n",
    "4. **Plot loadings heatmap** to understand how original features contribute to each principal component.\n",
    "5. **Visualize PCA scores by target classes** to see how the target variable (e.g., loan status) is distributed in PCA space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col_with_target = nominal_features + ordinal_features + target_variable\n",
    "print(cat_col_with_target)\n",
    "\n",
    "# Convert to string (supporting both list and string cases)\n",
    "target = target_variable[0] if isinstance(target_variable, list) else target_variable\n",
    "\n",
    "# Numerical and Categorical data frames without target variable\n",
    "cat_features_without_target = [col for col in cat_col_with_target if col != target]\n",
    "print(cat_features_without_target)\n",
    "\n",
    "num_features_df = cleaned_df.drop(columns=cat_col_with_target)\n",
    "categorical_df = cleaned_df[nominal_features + ordinal_features].drop(columns=target_variable)\n",
    "all_except_target_df = cleaned_df.drop(columns=target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb2e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_analyzer = PCAAnalyzer()\n",
    "pca_analyzer.fit(num_features_df)\n",
    "\n",
    "viz = DimensionalityViz(data=num_features_df,display_names=display_names)\n",
    "\n",
    "# To find how many PCs are needed to capture e.g. 90–95% of the variance.\n",
    "viz.plot_explained_variance(pca_analyzer.explained_variance)\n",
    "\n",
    "# Loadings show which original features contribute to each component.\n",
    "viz.plot_pca_loadings(pca_analyzer.loadings)\n",
    "\n",
    "# Visualize target classes in PCA space\n",
    "# PCA scores by target\n",
    "scores_df = pca_analyzer.get_scores_df()\n",
    "scores_df[target_variable[0]] = cleaned_df[target_variable[0]]  # Add target\n",
    "viz.plot_pca_scores(scores_df, target_column=target_variable[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e254b05b",
   "metadata": {},
   "source": [
    "##### 4.3.2 MCA Analysis on Categorical Features\n",
    "\n",
    "In this step, we perform Multiple Correspondence Analysis (MCA) to explore the structure of the categorical feature space to understand the relationships between different categories and how much \"inertia\" (similar to variance in PCA) is explained by the resulting dimensions.\n",
    "\n",
    "Steps:\n",
    "1. **Drop numerical features and the target variable** to ensure MCA only processes categorical data.\n",
    "2. **Fit MCA** on the cleaned categorical dataset.\n",
    "3. **Plot Explained Inertia** to visualize the cumulative explained inertia for the MCA dimensions.It helps in deciding how many dimensions are significant and should be retained for further analysis or interpretation\n",
    "4. **Plot MCA Column Coordinates (Category Contributions)** to visualize the coordinates of the original variable categories in the reduced MCA space. It reveals associations between categories \n",
    "5. **Visualize MCA Row Coordinates (Individual Scores) by Target Class** Plot the positions (coordinates) of each individual observation (row) in the reduced dimensional MCA space. This helps in understanding if the patterns captured by MCA among categorical features are related to the outcome variable.\n",
    "6. **Plot contribution heatmap** to understand how original features contribute to each principal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93bb17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCA on categorical data\n",
    "\n",
    "mca_analyzer = MCAAnalyzer(n_components=None)\n",
    "mca_analyzer.fit(categorical_df)\n",
    "\n",
    "print (mca_analyzer.get_explained_inertia())\n",
    "\n",
    "viz = DimensionalityViz(data=categorical_df, display_names=display_names)\n",
    "\n",
    "# Plot explained inertia (variance) to determine how many dimensions to retain\n",
    "viz.plot_mca_explained_inertia(mca_analyzer.get_explained_inertia())\n",
    "\n",
    "# MCA column coordinates: contributions of each category to the components\n",
    "viz.plot_mca_column_coordinates(mca_analyzer.get_column_coordinates_df())\n",
    "\n",
    "# MCA results by plotting the position of each row in the reduced dimensional space, colored by the target variable.\n",
    "row_coords = mca_analyzer.get_row_coordinates_df()\n",
    "row_coords[target_variable[0]] = cleaned_df[target_variable[0]]\n",
    "viz.plot_mca_row_coordinates(row_coords, target_column=target_variable[0])\n",
    "\n",
    "# Plot contributions of original variables to each principal \n",
    "contributions_df = mca_analyzer.get_column_contributions_df()\n",
    "viz.plot_mca_column_contributions(contributions_df)\n",
    "print(contributions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574190b",
   "metadata": {},
   "source": [
    "##### 4.3.3 FAMD Analysis on Mixed Features\n",
    "\n",
    "In this step, we perform **Factor Analysis of Mixed Data (FAMD)** to uncover latent patterns in datasets containing both **categorical and numerical** features. FAMD combines the strengths of PCA (for numerical data) and MCA (for categorical data) to jointly analyze mixed-type variables while ensuring balanced influence from each.\n",
    "\n",
    "---\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Prepare a mixed dataset**: Combine both **numerical** and **categorical** features, but exclude the **target variable** to avoid leakage during decomposition.\n",
    "\n",
    "2. **Fit FAMD** on the cleaned dataset using `FAMDAnalyzer`.\n",
    "\n",
    "3. **Plot Explained Inertia** to visualize the cumulative variance (inertia) explained by each FAMD dimension. This helps in selecting how many components are meaningful for interpretation.\n",
    "\n",
    "4. **Plot FAMD Column Coordinates (Feature Category Contributions)** to visualize how different feature categories or continuous variables are projected into the FAMD-reduced space. This reveals clustering and relationships across both variable types.\n",
    "\n",
    "5. **Visualize FAMD Row Coordinates (Individual Scores) by Target Class**: Plot the reduced coordinates for each data point and color by the target variable. This allows assessment of how well the FAMD components separate or cluster observations based on the outcome.\n",
    "\n",
    "6. **Plot Contribution Heatmap** to understand how much each original variable (categorical or numerical) contributes to each principal dimension. This is essential for interpretation of the latent components and identifying feature importance across the reduced space.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: FAMD automatically scales numerical features and encodes categorical variables internally. No manual preprocessing like standardization or one-hot encoding is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b66873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit FAMD on full (mixed) dataset\n",
    "famd_analyzer = FAMDAnalyzer(n_components=None,\n",
    "                             scale_numerical=True,        # Explicitly enabling\n",
    "                            scaling_method='zscore',      # Use scaling method\n",
    "                            handle_skew=True,             # Turn on skew correction\n",
    "                            skew_method='log',            # Use log transform\n",
    "                            skew_threshold=1.5            # Set a new threshold for skewness\n",
    "                            )\n",
    "famd_analyzer.fit(all_except_target_df)\n",
    "\n",
    "print(famd_analyzer.get_explained_inertia())\n",
    "\n",
    "# Set up the visualisation object\n",
    "viz = DimensionalityViz(data=all_except_target_df, display_names=display_names)\n",
    "\n",
    "# Plot explained inertia (variance) to determine how many components to retain\n",
    "viz.plot_famd_explained_inertia(famd_analyzer.get_explained_inertia())\n",
    "\n",
    "# Plot column coordinates (showing variable positions in FAMD space)\n",
    "viz.plot_famd_column_coordinates(famd_analyzer.get_column_coordinates_df())\n",
    "\n",
    "# Plot row coordinates (samples) colored by the target variable\n",
    "row_coords = famd_analyzer.get_row_coordinates_df()\n",
    "row_coords[target_variable[0]] = cleaned_df[target_variable[0]]\n",
    "viz.plot_famd_row_coordinates(row_coords, target_column=target_variable[0])\n",
    "\n",
    "# Plot contributions of original variables to each FAMD dimension\n",
    "contributions_df = famd_analyzer.get_contributions()\n",
    "viz.plot_famd_column_contributions(contributions_df)\n",
    "print(contributions_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f511c4b9",
   "metadata": {},
   "source": [
    "#### 4.4 Clustering Analysis\n",
    "This section investigates natural groupings or underlying patterns in the data by applying clustering techniques. The objective is to uncover homogeneous subgroups within the dataset that may not be visible through standard univariate or bivariate analysis.\n",
    "We explore clustering using:\n",
    "- Numerical features: K-Means, Hierarchical, and DBSCAN Clustering analysis \n",
    "- Categorical features\n",
    "- Combined numerical and categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6c8bfe",
   "metadata": {},
   "source": [
    "##### 4.4.1 Clustering Analysis for Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b95f8e",
   "metadata": {},
   "source": [
    "##### K-Means Clustering Analysis\n",
    "We applied K-Means clustering to segment the dataset based on numerical features. The process included:\n",
    "- Scaling and log transformation to handle skewness.\n",
    "- Grid search for optimal number of clusters using Silhouette scores (Optional).\n",
    "- Dimensionality reduction using PCA for 2D visualization.\n",
    "- Cluster profiling to interpret group characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13da8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the KMeansClustering class\n",
    "# kmeans_analyser = KMeansClustering(df = cleaned_df,features=numerical_features,n_clusters=2,\n",
    "#                                    scale=True, handle_skew=True, skew_method='log', \n",
    "#                                    skew_threshold=1, random_state=42, \n",
    "#                                    tune_mode=None, # Optional: None or 'grid' for grid search\n",
    "#                                    cluster_range=list(range(2, 11)))\n",
    "\n",
    "# print(kmeans_analyser.grid_search_results)\n",
    "\n",
    "# # Fit and predict cluster labels\n",
    "# labels = kmeans_analyser.fit_predict()\n",
    "\n",
    "# # Get 2D PCA projection of the clustered data\n",
    "# pca_components = kmeans_analyser.transform_pca()\n",
    "\n",
    "# # Project centroids into PCA space\n",
    "# centroids_scaled = kmeans_analyser.get_centroids()\n",
    "# centroids_pca = kmeans_analyser.pca_.transform(centroids_scaled)\n",
    "\n",
    "\n",
    "# # Visualize clusters in 2D\n",
    "# ClusterVisualisation.plot_clusters_2d(\n",
    "#     components=pca_components,\n",
    "#     labels=labels,\n",
    "#     centroids=centroids_pca,\n",
    "#     title='K-Means Clustering (PCA Projection)',\n",
    "#     xlabel=\"PCA Component 1\", ylabel=\"PCA Component 2\"\n",
    "# )\n",
    "\n",
    "# # Profile clusters\n",
    "# pivoted_profile = kmeans_analyser.profile_clusters(pivot=True)\n",
    "\n",
    "# save_utils.save_dataframe_to_csv(pivoted_profile, os.path.join(output_path, \"kmeans_analysis.csv\"), overwrite=True, index=True)\n",
    "# # print(pivoted_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc194a",
   "metadata": {},
   "source": [
    "##### Hierarchical Clustering Analysis (Numerical Features - PCA)\n",
    "We applied Hierarchical clustering to segment the dataset based on numerical features. The process included:\n",
    "- Scaling and log transformation to handle skewness.\n",
    "- Grid search for optimal number of clusters using Silhouette scores (Optional).\n",
    "- Dimensionality reduction using PCA for 2D visualization.\n",
    "- Cluster profiling to interpret group characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b81abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the HierarchicalClustering class\n",
    "# hierarchical_analyser = HierarchicalClustering(\n",
    "#     df=cleaned_df,\n",
    "#     features=numerical_features,\n",
    "#     scale=True,\n",
    "#     handle_skew=True,\n",
    "#     skew_method='log',\n",
    "#     skew_threshold=1.0,\n",
    "#     linkage='ward',       # For ward, metric must be 'euclidean'\n",
    "#     metric='euclidean',\n",
    "#     tune_mode=None,     # Optional: None or 'grid' for grid search  \n",
    "#     cluster_range=list(range(2, 11)),\n",
    "#     n_clusters=3,    # default in case grid not used\n",
    "#     mca_mode=False\n",
    "# )\n",
    "\n",
    "# # Fit and predict cluster labels\n",
    "# labels = hierarchical_analyser.fit_predict()\n",
    "\n",
    "# # Show grid results\n",
    "# print(hierarchical_analyser.grid_search_results)\n",
    "\n",
    "# # Get 2D PCA projection of the clustered data\n",
    "# pca_components = hierarchical_analyser.project_2d()\n",
    "\n",
    "# # Hierarchical clustering doesn't have centroids, but we can use mean positions per cluster\n",
    "# centroids_pca = hierarchical_analyser.get_cluster_means_in_2d_space(pca_components)\n",
    "\n",
    "# # Visualize clusters in 2D\n",
    "# ClusterVisualisation.plot_clusters_2d(\n",
    "#     components=pca_components,\n",
    "#     labels=labels,\n",
    "#     centroids=centroids_pca,\n",
    "#     title='Hierarchical Clustering (PCA Projection)',\n",
    "#     xlabel=\"PCA Component 1\", ylabel=\"PCA Component 2\"\n",
    "# )\n",
    "\n",
    "# # Profile clusters\n",
    "# pivoted_profile = hierarchical_analyser.profile_clusters(pivot=True)\n",
    "\n",
    "\n",
    "# # Save cluster profiling results\n",
    "# save_utils.save_dataframe_to_csv(\n",
    "#     pivoted_profile,\n",
    "#     os.path.join(output_path, \"hierarchical_analysis_pca.csv\"),\n",
    "#     overwrite=True,\n",
    "#     index=True\n",
    "# )\n",
    "\n",
    "# # Optionally print\n",
    "# # print(pivoted_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfeec50",
   "metadata": {},
   "source": [
    "##### DBSCAN Clustering analysis\n",
    "We applied DBSCAN clustering to segment the dataset based on numerical features. The process included:\n",
    "- Scaling and log transformation to handle skewness.\n",
    "- Grid search for optimal number of clusters using Silhouette scores (Optional).\n",
    "- Dimensionality reduction using PCA for 2D visualization.\n",
    "- Cluster profiling to interpret group characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d12d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the DBSCANClustering class\n",
    "# dbscan_analyser = DBSCANClustering(\n",
    "#     df=cleaned_df,\n",
    "#     features=numerical_features,\n",
    "#     eps='auto',                  # Distance threshold for clustering, \"auto\" for tuning or fixed input such as 0.3, 0.5, 0.7, 1.0\n",
    "#     min_samples=5,               # Minimum samples per core point\n",
    "#     metric='euclidean',\n",
    "#     scale=True,                  # Whether to scale features\n",
    "#     handle_skew=True,            # Whether to reduce skewness\n",
    "#     skew_method='log',           # Method of skewness reduction\n",
    "#     skew_threshold=1.0,          # Skewness threshold to apply transformation\n",
    "#     scaling_method='zscore',     # Scaling method: 'zscore' or 'minmax'\n",
    "#     suggest_eps_percentile = 90,  # For tuning eps\n",
    "#     tune_mode=None,  # Options: None, 'eps', 'tune_min_samples', 'grid'\n",
    "#     min_samples_range=None,\n",
    "#     eps_percentiles=None\n",
    "# )\n",
    "\n",
    "# # print(dbscan_analyser.grid_search_results)\n",
    "\n",
    "# # Fit and predict cluster labels\n",
    "# labels = dbscan_analyser.fit_predict()\n",
    "\n",
    "# # Get 2D PCA projection of the clustered data\n",
    "# pca_components = dbscan_analyser.project_pca()\n",
    "\n",
    "# # DBSCAN doesn’t compute centroids, but we can calculate average positions per cluster (ignoring noise)\n",
    "# centroids_pca = dbscan_analyser.get_cluster_means_in_pca_space(pca_components)\n",
    "\n",
    "# # Visualize clusters in 2D\n",
    "# ClusterVisualisation.plot_clusters_2d(\n",
    "#     components=pca_components,\n",
    "#     labels=labels,\n",
    "#     centroids=centroids_pca,\n",
    "#     title='DBSCAN Clustering (PCA Projection)',\n",
    "#     xlabel=\"PCA Component 1\", ylabel=\"PCA Component 2\"\n",
    "# )\n",
    "\n",
    "# # Profile clusters (excluding noise if your logic skips -1)\n",
    "# pivoted_profile = dbscan_analyser.profile_clusters(pivot=True)\n",
    "\n",
    "# # Save cluster profiling results\n",
    "# save_utils.save_dataframe_to_csv(\n",
    "#     pivoted_profile,\n",
    "#     os.path.join(output_path, \"dbscan_analysis.csv\"),\n",
    "#     overwrite=True,\n",
    "#     index=True\n",
    "# )\n",
    "\n",
    "# # Optionally print\n",
    "# # print(pivoted_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66bbc70",
   "metadata": {},
   "source": [
    "##### 4.4.2 Clustering Analysis for Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146e8dd",
   "metadata": {},
   "source": [
    "##### K-Modes Clustering Analysis (Categorical Features - MCA)\n",
    "We applied K-Modes clustering to segment the dataset based purely on categorical features, leveraging MCA for visualization. The process included:\n",
    "\n",
    "Clustering using the K-Modes algorithm, which uses modes instead of means and a simple matching dissimilarity for categorical attributes.\n",
    "\n",
    "Optional grid search to determine the best number of clusters and initialization method (Huang or Cao) using internal validation scores.\n",
    "\n",
    "Dimensionality reduction with Multiple Correspondence Analysis (MCA) to project the clusters into 2D space for visualization.\n",
    "\n",
    "Cluster visualization in MCA space alongside projected centroids for interpretability.\n",
    "\n",
    "Detailed cluster profiling based on the distribution of original categorical features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066cbda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the KModesClustering class\n",
    "# kmodes_analyser = KModesClustering(\n",
    "#     df=cleaned_df,\n",
    "#     features= cat_features_without_target,   # List of categorical columns excluding target variable\n",
    "#     n_clusters=3,                            # or leave it out and set tune_mode='grid'\n",
    "#     init='Huang',                            # or 'Cao'\n",
    "#     n_init=10,\n",
    "#     verbose=0,\n",
    "#     random_state=42,\n",
    "#     tune_mode=None,                         # Optional: None or 'grid' for grid search  \n",
    "#     cluster_range=list(range(2, 6)),        # Optional: range of clusters to test\n",
    "#     init_methods=['Huang', 'Cao']           # Optional: init methods to test\n",
    "# )\n",
    "\n",
    "# # Fit and predict cluster labels\n",
    "# labels = kmodes_analyser.fit_predict()\n",
    "\n",
    "# # Project data and centroids to MCA space\n",
    "# mca_components = np.asarray(kmodes_analyser.project_mca(), dtype=float)\n",
    "# centroids_mca = np.asarray(kmodes_analyser.project_centroids_to_mca(), dtype=float)\n",
    "\n",
    "\n",
    "# # Visualize clusters in 2D\n",
    "# ClusterVisualisation.plot_clusters_2d(\n",
    "#     components=mca_components,\n",
    "#     labels=kmodes_analyser.labels_,\n",
    "#     centroids=centroids_mca,\n",
    "#     title='KModes Clustering (MCA Projection)',\n",
    "#     xlabel=\"MCA Component 1\", ylabel=\"MCA Component 2\"\n",
    "# )\n",
    "\n",
    "# # Profile clusters\n",
    "# cluster_profile = kmodes_analyser.profile_clusters(include_counts=True)\n",
    "\n",
    "# # Save cluster profiling to CSV\n",
    "# save_utils.save_dataframe_to_csv(\n",
    "#     cluster_profile,\n",
    "#     os.path.join(output_path, \"kmodes_analysis.csv\"),\n",
    "#     overwrite=True,\n",
    "#     index=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2dcc05",
   "metadata": {},
   "source": [
    "##### Hierarchical Clustering Analysis (Categorical Features - MCA)\n",
    "We applied Hierarchical clustering to segment the dataset based on **categorical features** using MCA (Multiple Correspondence Analysis). The process included:\n",
    "- Dimensionality reduction using MCA to project categorical data into a numeric space.\n",
    "- Optional grid search to identify the optimal number of clusters using Silhouette scores.\n",
    "- Clustering using Agglomerative Hierarchical Clustering.\n",
    "- Cluster profiling based on the original categorical feature distribution to interpret segment characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb34ff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the HierarchicalClustering class\n",
    "# hierarchical_analyser = HierarchicalClustering(\n",
    "#     df=cleaned_df,\n",
    "#     features=cat_features_without_target,\n",
    "#     scale=False,\n",
    "#     handle_skew=False,\n",
    "#     skew_method='log',\n",
    "#     skew_threshold=1.0,\n",
    "#     linkage='ward',       # For ward, metric must be 'euclidean'\n",
    "#     metric='euclidean',\n",
    "#     tune_mode=None,     # Optional: None or 'grid' for grid search  \n",
    "#     cluster_range=list(range(2, 11)),\n",
    "#     n_clusters=3,    # default in case grid not used\n",
    "#     mca_mode=True\n",
    "# )\n",
    "\n",
    "# # Fit and predict cluster labels\n",
    "# labels = hierarchical_analyser.fit_predict()\n",
    "\n",
    "# # Show grid results\n",
    "# print(hierarchical_analyser.grid_search_results)\n",
    "\n",
    "# # Get 2D PCA projection of the clustered data\n",
    "# mca_components = hierarchical_analyser.project_2d()\n",
    "\n",
    "# # Approximate cluster centers in MCA space\n",
    "# centroids_mca = hierarchical_analyser.get_cluster_means_in_2d_space(mca_components)\n",
    "\n",
    "# # Visualize clusters in 2D\n",
    "# ClusterVisualisation.plot_clusters_2d(\n",
    "#     components=mca_components,\n",
    "#     labels=labels,\n",
    "#     centroids=centroids_mca,\n",
    "#     title='Hierarchical Clustering (MCA Projection)',\n",
    "#     xlabel=\"MCA Component 1\", ylabel=\"MCA Component 2\"\n",
    "# )\n",
    "\n",
    "# # Profile clusters\n",
    "# pivoted_profile = hierarchical_analyser.profile_clusters(pivot=True)\n",
    "\n",
    "\n",
    "# # Save cluster profiling results\n",
    "# save_utils.save_dataframe_to_csv(\n",
    "#     pivoted_profile,\n",
    "#     os.path.join(output_path, \"hierarchical_analysis_mca.csv\"),\n",
    "#     overwrite=True,\n",
    "#     index=True\n",
    "# )\n",
    "\n",
    "# # Optionally print\n",
    "# # print(pivoted_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9098115c",
   "metadata": {},
   "source": [
    "##### 4.4.3 Clustering Analysis for Mixed (Numerical + Categorical) Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11a9abb",
   "metadata": {},
   "source": [
    "##### K-Prototypes Clustering Analysis (Mixed Features - FAMD)\n",
    "We applied **K-Prototypes clustering** to segment the dataset based on a mix of **categorical and numerical features**, using **FAMD** for dimensionality reduction. The process included:\n",
    "\n",
    "- **Dimensionality reduction** using **FAMD (Factor Analysis of Mixed Data)** to project both categorical and numerical features into a shared numerical space for visualization.\n",
    "- **Optional grid search** to identify the optimal number of clusters based on **Silhouette scores**.\n",
    "- **Clustering** using the **K-Prototypes algorithm**, which handles mixed data types efficiently.\n",
    "- **Cluster profiling** to interpret segment characteristics by analyzing the distribution of original features across clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bc5207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the KPrototypesClustering class\n",
    "# kproto_analyser = KPrototypesClustering(\n",
    "#                 cleaned_df, cat_features_without_target, numerical_features,\n",
    "#                  n_clusters=3, init='Huang', n_init=10, verbose=0,\n",
    "#                  random_state=42, \n",
    "#                  tune_mode=None, # Optional: None or 'grid'\n",
    "#                  cluster_range=None,\n",
    "#                  init_methods=['Huang', 'Cao'],\n",
    "#                  scale=True, scaling_method='zscore',\n",
    "#                  handle_skew=True, skew_method='log', skew_threshold=1.0)\n",
    "\n",
    "\n",
    "# # Fit and predict cluster labels\n",
    "# labels = kproto_analyser.fit_predict()\n",
    "\n",
    "# # Show grid search results (if any)\n",
    "# print(kproto_analyser.grid_search_results)\n",
    "\n",
    "# # Get 2D FAMD projection\n",
    "# famd_components = kproto_analyser.project_clusters_famd()\n",
    "\n",
    "# # Get cluster centers in FAMD space\n",
    "# centroids_famd = kproto_analyser.get_cluster_means_in_2d_space(famd_components)\n",
    "\n",
    "# # Visualize clusters in 2D using the utility class\n",
    "# ClusterVisualisation.plot_clusters_2d(\n",
    "#     components=famd_components,\n",
    "#     labels=labels,\n",
    "#     centroids=centroids_famd,\n",
    "#     title='K-Prototypes Clustering (FAMD Projection)',\n",
    "#     xlabel=\"FAMD Component 1\", ylabel=\"FAMD Component 2\"\n",
    "# )\n",
    "\n",
    "# # Profile clusters and pivot\n",
    "# pivoted_profile = kproto_analyser.profile_clusters()\n",
    "\n",
    "# # Save the profiling results\n",
    "# save_utils.save_dataframe_to_csv(\n",
    "#     pivoted_profile,\n",
    "#     os.path.join(output_path, \"kprototypes_analysis.csv\"),\n",
    "#     overwrite=True,\n",
    "#     index=True\n",
    "# )\n",
    "\n",
    "# # Optionally print\n",
    "# print(pivoted_profile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c6124",
   "metadata": {},
   "source": [
    "##### Hierarchical Clustering Analysis with Gower Distance (Mixed Features - FAMD Visualization)\n",
    "To segment the dataset based on its combination of **categorical and numerical features**, we employed **Hierarchical Agglomerative Clustering** coupled with the **Gower distance** metric.\n",
    "Visualization of these clusters was facilitated using **FAMD**. The analytical process encompassed:\n",
    "\n",
    "- **Similarity Measurement**: Computation of a Gower distance matrix to effectively quantify similarities between data points across mixed data types, appropriately handling numerical and categorical attributes.\n",
    "- **Clustering Algorithm**: Application of Hierarchical Agglomerative Clustering on the Gower distance matrix. This method builds a hierarchy of clusters (dendrogram) without requiring a pre-specified number of clusters initially.\n",
    "- **Optimal Cluster Determination**: Optional tuning to identify the most suitable number of clusters (by selecting a level to cut the dendrogram) and the optimal linkage method (e.g., 'average', 'complete') by evaluating Silhouette scores.\n",
    "- **Dimensionality Reduction for Visualization**: Utilization of FAMD (Factor Analysis of Mixed Data) to project the original dataset (both categorical and numerical features) into a lower-dimensional space, enabling 2D visualization of the derived cluster structures.\n",
    "- **Cluster Profiling**: Interpretation of the resulting segments by analyzing the distribution and central tendencies (e.g., means for numerical, modes for categorical) of the original features within each identified cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c747eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the HierarchicalGowerClustering class\n",
    "# hierarchical_analyser = HierarchicalGowerClustering(\n",
    "#     df=cleaned_df,\n",
    "#     categorical_features=cat_features_without_target,\n",
    "#     numerical_features=numerical_features,\n",
    "#     n_clusters=3,  # Example: specify desired k, or use tune_mode\n",
    "#     linkage_method='average', # Common linkage method\n",
    "#     verbose=1, # Set to 1 or higher for more output\n",
    "#     random_state=42,\n",
    "#     tune_mode=None, # Optional: 'silhouette' or None\n",
    "#     cluster_range=list(range(2, 7)), # Range of k to test if tune_mode='silhouette'\n",
    "#     linkage_methods_to_tune=['average', 'complete'], # Linkage methods to test if tune_mode='silhouette'\n",
    "#     scale=True,\n",
    "#     scaling_method='zscore',\n",
    "#     handle_skew=True,\n",
    "#     skew_method='log',\n",
    "#     skew_threshold=1.0\n",
    "# )\n",
    "\n",
    "# labels_hierarchical = hierarchical_analyser.labels_\n",
    "\n",
    "# # Show tuning results (if any)\n",
    "# if hierarchical_analyser.tuning_results_ is not None:\n",
    "#     print(\"\\n--- Hierarchical Clustering Tuning Results ---\")\n",
    "#     print(hierarchical_analyser.tuning_results_)\n",
    "#     print(f\"Best n_clusters found: {hierarchical_analyser.n_clusters}\")\n",
    "#     print(f\"Best linkage method found: {hierarchical_analyser.linkage_method}\")\n",
    "\n",
    "# # Get 2D FAMD projection\n",
    "# famd_components_hierarchical = hierarchical_analyser.project_clusters_famd()\n",
    "\n",
    "# # Get cluster centers in FAMD space\n",
    "# centroids_famd_hierarchical = hierarchical_analyser.get_cluster_means_in_2d_space(famd_components_hierarchical)\n",
    "\n",
    "# # Visualize clusters in 2D using the utility class\n",
    "# ClusterVisualisation.plot_clusters_2d(\n",
    "#     components=famd_components_hierarchical, # DataFrame with FAMD components and 'Cluster' column\n",
    "#     labels=labels_hierarchical, # Pass the labels explicitly or ensure components DF has 'Cluster'\n",
    "#     centroids=centroids_famd_hierarchical, # DataFrame of 2D centroids, indexed by cluster label\n",
    "#     title='Hierarchical Clustering with Gower Distance (FAMD Projection)',\n",
    "#     xlabel=\"FAMD Component 1\",\n",
    "#     ylabel=\"FAMD Component 2\"\n",
    "# )\n",
    "\n",
    "# # Profile clusters\n",
    "# profile_hierarchical = hierarchical_analyser.profile_clusters()\n",
    "\n",
    "# # Save the profiling results\n",
    "# save_utils.save_dataframe_to_csv(\n",
    "#     profile_hierarchical,\n",
    "#     os.path.join(output_path, \"hierarchical_gower_analysis.csv\"),\n",
    "#     overwrite=True,\n",
    "#     index=True # Cluster profiles are typically indexed by cluster label\n",
    "# )\n",
    "\n",
    "# # Optionally print\n",
    "# print(\"\\n--- Hierarchical Gower Clustering Profile ---\")\n",
    "# print(profile_hierarchical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85db303",
   "metadata": {},
   "source": [
    "#### Step 5: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17bfd3",
   "metadata": {},
   "source": [
    "##### 5.1 Feature Importance\n",
    "This section implements methods to evaluate feature importance for predicting loan default (PD).\n",
    "It includes both traditional statistical and machine learning-based approaches:\n",
    "- Logistic Regression using Weight of Evidence (WoE) and Information Value (IV)\n",
    "- Tree-based methods such as Random Forest for model-based feature importance\n",
    "\n",
    "These methods guide variable selection by identifying the most predictive features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac106521",
   "metadata": {},
   "source": [
    "##### 5.1.1 Logistic Regression\n",
    "To evaluate the predictive power of features collectively, we use Logistic Regression — a statistical model well-suited for binary classification tasks such as default prediction.\n",
    "\n",
    "##### 5.1.1.1 Binning\n",
    "\n",
    "Applies binning to numerical and categorical variables as a preprocessing step for WoE transformation.\n",
    "Supports manual and algorithmic methods such as equal-width, quantile, decile, decision trees, K-means, and optimal binning.\n",
    "\n",
    "Binning transforms continuous variables into discrete intervals, improving interpretability and performance in scorecard models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96c2ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define binning configuration\n",
    "testing_binning_config = {\n",
    "    \"person_age\": {\n",
    "        \"type\": \"numerical\",\n",
    "        \"method\": \"optimal\",  #  can change this to 'quantile', 'decile', etc.\n",
    "        \"target\": \"loan_status\"  # only needed for supervised methods like 'optimal'\n",
    "    },\n",
    "    \"cb_person_cred_hist_length\": {\n",
    "        \"type\": \"numerical\",\n",
    "        \"method\": \"optimal\", \n",
    "        \"target\": \"loan_status\"  \n",
    "    },\n",
    "    \"person_income\": {\n",
    "        \"type\": \"numerical\",\n",
    "        \"method\": \"optimal\", \n",
    "        \"target\": \"loan_status\"\n",
    "    },\n",
    "    \"person_emp_length\": {\n",
    "        \"type\": \"numerical\",\n",
    "        \"method\": \"optimal\",\n",
    "        \"target\": \"loan_status\"\n",
    "    },\n",
    "    \"loan_amnt\": {\n",
    "        \"type\": \"numerical\",\n",
    "        \"method\": \"optimal\",\n",
    "        \"target\": \"loan_status\"\n",
    "    },\n",
    "    \"loan_int_rate\": {\n",
    "        \"type\": \"numerical\",\n",
    "        \"method\": \"optimal\", \n",
    "        \"target\": \"loan_status\"\n",
    "    },\n",
    "    \"loan_percent_income\": {\n",
    "        \"type\": \"numerical\",\n",
    "        \"method\": \"optimal\", \n",
    "        \"target\": \"loan_status\"\n",
    "}\n",
    "}\n",
    "\n",
    "# Step 2: Initialize the Binner class \n",
    "binner = Binner(df=cleaned_df, config=testing_binning_config)\n",
    "\n",
    "# Step 3: Suggest bins only (if apply=False does NOT apply the bins to the DataFrame)\n",
    "binner.suggest_and_apply_bins(features_to_bin=[\"person_age\", \"cb_person_cred_hist_length\",\n",
    "                                     \"person_income\",\"person_emp_length\",\"loan_amnt\",\n",
    "                                     \"loan_int_rate\",\"loan_percent_income\"], apply=False)\n",
    "\n",
    "# Step 4: Access the suggested bin edges for review\n",
    "print(\"\\n--- Suggested Bin Edges ---\")\n",
    "for feature in binning_config.keys():\n",
    "    bin_edges = binner.get_bin_edges(feature)\n",
    "    print(f\"{feature}: {bin_edges}\")\n",
    "\n",
    "# To see all binning info\n",
    "print(binner.get_binning_info())\n",
    "\n",
    "# Step 5: Apply the bins and create new columns\n",
    "df_binned = binner.apply_bins()\n",
    "print(df_binned.head(10))\n",
    "print(df_binned.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aa545e",
   "metadata": {},
   "source": [
    "##### Applying Manual Binning from a Data Source (CSV, Excel, SQL, etc.)\n",
    "This section explains how to use manually defined binning rules (e.g., bin edges or category mappings) stored in an external data source like a CSV, Excel, or SQL table to transform features accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33ea7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "binner = Binner(df=cleaned_df, config=binning_config)\n",
    "df_binned = binner.suggest_and_apply_bins(apply=True)\n",
    "print(df_binned.head(10))\n",
    "print(df_binned.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc9e62",
   "metadata": {},
   "source": [
    "Saving the new data frame and also visulalising the bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a CSV file\n",
    "save_utils = SaveUtils()\n",
    "save_utils.save_dataframe_to_csv(df_binned, os.path.join(output_path, \"loan_data_binned.csv\"), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = Visualisation(df_binned, display_names)\n",
    "viz.plot_binned_distribution(\"cb_person_cred_hist_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213b3b9",
   "metadata": {},
   "source": [
    "##### 5.1.1.2 Calulating Weight of Evidence (WoE)\n",
    "Weight of Evidence (WoE) transforms binned features into interpretable numerical values that reflect the strength and direction of their relationship with the target variable. It is especially useful in credit risk modeling as it helps improve model interpretability and supports monotonic relationships with default likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Define your binned features and target\n",
    "binned_features = ['person_age_binned', 'person_income_binned', 'person_emp_length_binned', 'loan_amnt_binned', 'loan_int_rate_binned', 'loan_percent_income_binned', 'cb_person_cred_hist_length_binned']\n",
    "target = target_variable\n",
    "\n",
    "# Step 2: Initialize the encoder\n",
    "woe_encoder = WOEEncoder(df_binned, target)\n",
    "\n",
    "# Step 3: Fit on binned features\n",
    "woe_encoder.fit(binned_features)\n",
    "\n",
    "# Optional: See the WOE mapping\n",
    "woe_map = woe_encoder.get_woe_mapping('cb_person_cred_hist_length_binned')\n",
    "print(woe_map)\n",
    "\n",
    "# Step 4: Apply WOE transformation\n",
    "woe_encoder.transform()  # or .transform(['loan_amnt_binned']) for a subset\n",
    "\n",
    "# Step 5: Get the final DataFrame\n",
    "woe_df = woe_encoder.get_transformed_data()\n",
    "\n",
    "print(woe_df.head(10))\n",
    "print(woe_df.columns.tolist())\n",
    "\n",
    "# Optional: Save dataframe\n",
    "save_utils.save_dataframe_to_csv(woe_df, os.path.join(output_path, \"loan_data_woe.csv\"), overwrite=True)\n",
    "\n",
    "# Now we can use `woe_df` for:\n",
    "# - correlation matrix\n",
    "# - training a logistic regression\n",
    "# - plotting WOE vs default rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca9141e",
   "metadata": {},
   "source": [
    "Visualising the Weight of Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce194f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "woe_map = woe_encoder.get_woe_mapping()\n",
    "\n",
    "viz = FeatureImportanceVisualiser(woe_map=woe_map)\n",
    "\n",
    "# Plot for a single feature\n",
    "viz.plot_woe_trend(\"loan_amnt_binned\")\n",
    "\n",
    "# Plot for all features\n",
    "# viz.plot_all_woe_trends()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95151d3",
   "metadata": {},
   "source": [
    "##### 5.1.1.3 Calulating Information Value (IV)\n",
    "Information Value (IV) is used to measure the predictive power of each binned feature in relation to the binary target variable. Higher IV indicates stronger separation between good and bad outcomes.\n",
    "\n",
    "- **Steps**:\n",
    "  1. Initialize the `IVCalculator` with the DataFrame and target column.\n",
    "  2. Compute IV for all binned features using either precomputed WoE values or calculating WoE internally.\n",
    "  3. Convert results to a DataFrame and visualize.\n",
    "\n",
    "- **Interpretation Guide**:\n",
    "  - IV < 0.02: Not useful\n",
    "  - 0.02 ≤ IV < 0.1: Weak predictor\n",
    "  - 0.1 ≤ IV < 0.3: Medium predictor\n",
    "  - 0.3 ≤ IV < 0.5: Strong predictor\n",
    "  - IV ≥ 0.5: Suspiciously strong (may indicate overfitting or data leakage)\n",
    "\n",
    "The resulting IV scores help in selecting the most informative features for credit risk modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f813c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize\n",
    "iv_calc = IVCalculator(woe_df, target=target)\n",
    "\n",
    "# Step 2: Run IV calculation\n",
    "iv_calc.calculate_iv(\n",
    "    binned_features=[\n",
    "        'loan_amnt_binned', 'person_age_binned', 'person_income_binned',\n",
    "        'person_emp_length_binned','loan_int_rate_binned','loan_percent_income_binned',\n",
    "        'cb_person_cred_hist_length_binned'\n",
    "    ],\n",
    "\n",
    "    woe_column_map={\n",
    "        'loan_amnt_binned': 'loan_amnt_binned_woe',\n",
    "        'person_age_binned': 'person_age_binned_woe',\n",
    "        'person_income_binned': 'person_income_binned_woe',\n",
    "        'person_emp_length_binned' : 'person_emp_length_binned_woe',\n",
    "        'loan_int_rate_binned' : 'loan_int_rate_binned_woe',\n",
    "        'loan_percent_income_binned' : 'loan_percent_income_binned_woe',\n",
    "        'cb_person_cred_hist_length_binned' : 'cb_person_cred_hist_length_binned_woe'\n",
    "    },\n",
    "    use_precomputed_woe=True\n",
    ")\n",
    "\n",
    "# Step 3: Get IV scores\n",
    "iv_scores_df = iv_calc.as_dataframe()\n",
    "print(iv_scores_df)\n",
    "\n",
    "viz.plot_iv_scores(iv_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c521687",
   "metadata": {},
   "source": [
    "##### 5.1.1.4 Encoding Nominal and Ordinal Variables\n",
    "To prepare categorical variables for logistic regression and other ML models, we applied appropriate encoding techniques:\n",
    "\n",
    "Nominal variables (e.g., loan_intent, person_home_ownership) were encoded using One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = DataEncoder(\n",
    "    nominal_features=['person_home_ownership', 'loan_intent', 'loan_grade'],\n",
    "    ordinal_features=None,drop_original=False\n",
    ")\n",
    "\n",
    "encoded_df = encoder.fit_transform(woe_df)\n",
    "\n",
    "# Turn \"cb_person_default_on_file\" to a Boolean variable \n",
    "encoded_df['cb_person_default_on_file'] = encoded_df['cb_person_default_on_file'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "print(encoded_df.head(10))\n",
    "print(encoded_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9215798",
   "metadata": {},
   "source": [
    "##### 5.1.1.5 Calculate Feature Importance with Logistic Regression (Logit)\n",
    "\n",
    "Unlike Information Value (IV), which evaluates individual variable relationships with the target, logistic regression captures joint effects, including multicollinearity and interactions between features.\n",
    "\n",
    "In this step:\n",
    "\n",
    "- We train a logistic regression model on the preprocessed dataset (with WoE-transformed numerical features and one-hot encoded categorical variables).\n",
    "\n",
    "- We analyze the resulting coefficients:\n",
    "\n",
    "  - Magnitude and sign of each coefficient indicate the direction and strength of influence on the target.\n",
    "\n",
    "  - Larger absolute values imply stronger predictive contribution.\n",
    "\n",
    "- We evaluate models beyond coefficients by calculating permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c7988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [\n",
    "    'loan_amnt_binned_woe','person_age_binned_woe','person_income_binned_woe','person_emp_length_binned_woe',\n",
    "    'loan_int_rate_binned_woe','loan_percent_income_binned_woe','cb_person_cred_hist_length_binned_woe',\n",
    "    'person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', \n",
    "    'person_home_ownership_RENT', 'loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', \n",
    "    'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', \n",
    "    'loan_intent_VENTURE', 'loan_grade_A', 'loan_grade_B', 'loan_grade_C', 'loan_grade_D',\n",
    "    'loan_grade_E', 'loan_grade_F', 'loan_grade_G', 'cb_person_default_on_file'\n",
    "]\n",
    "\n",
    "# Step 1: Initialize and train the model\n",
    "model = LogisticModel(\n",
    "    df=encoded_df,\n",
    "    features=features,\n",
    "    target=target,\n",
    "    id_column=None,                # or 'id' if we have one\n",
    "    test_size=0.2,\n",
    "    eval_size=0.0,                 # or set to 0.1 if we want an eval set\n",
    "    random_state=42,\n",
    "    balance_method='none',        # or 'smote', 'undersample', 'oversample'\n",
    "    tune_hyperparameters=False,   # Set True if we want GridSearch\n",
    "    scoring='roc_auc',\n",
    "    solver='liblinear',           # Or 'saga', 'lbfgs', etc.\n",
    "    scale=False,\n",
    "    scaling_method='zscore',\n",
    "    handle_skew=True,\n",
    "    skew_method='log',\n",
    "    skew_threshold=1.0\n",
    ")\n",
    "\n",
    "# Step 2: Train the model\n",
    "model.fit_model()\n",
    "\n",
    "# Step 3: Get sorted feature coefficients\n",
    "coeff_df = model.get_coefficients()\n",
    "print(\"🔹 Logit Coefficient:\\n\", coeff_df.head(30))  \n",
    "\n",
    "# Step 4: Get permutation importance\n",
    "importance_df = model.get_permutation_importance(dataset='test', scoring='roc_auc')\n",
    "print(\"🔹 Permutation Importance:\\n\",importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b24b45",
   "metadata": {},
   "source": [
    "#### 5.1.2 Calculate Feature Importance with Random Forest\n",
    "Random Forest is a powerful ensemble learning method that captures complex patterns, interactions, and non-linear relationships across features without requiring explicit transformations.\n",
    "\n",
    "In this step:\n",
    "\n",
    "- We train a Random Forest classifier on the preprocessed dataset (with WoE-transformed numerical features and one-hot encoded categorical variables).\n",
    "\n",
    "- We evaluate feature importance using three complementary methods:\n",
    "\n",
    "    - Gini Importance (Mean Decrease in Impurity):\n",
    "    Measures how much each feature decreases node impurity across all trees in the forest. Features with higher scores are considered more important. However, this method can be biased toward features with many unique values.\n",
    "\n",
    "    - Permutation Importance:\n",
    "    Assesses the drop in model performance (e.g., ROC AUC) when the values of a feature are randomly shuffled. This helps reveal the true predictive contribution of each feature in the trained model and avoids structural bias.\n",
    "\n",
    "    - SHAP Values (SHapley Additive exPlanations):\n",
    "    Based on cooperative game theory, SHAP provides local and global explanations by quantifying the average contribution of each feature to predictions. It captures both magnitude and direction, offering the most detailed and model-consistent interpretation.\n",
    "\n",
    "Using all three methods allows us to evaluate importance from different angles: impurity reduction, model dependence, and individual feature attribution — leading to a more comprehensive understanding of feature behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd5969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WoE features are not used for random forest. Here we use original vaues instead of them.\n",
    "\n",
    "original_features = [ 'person_age', 'person_income', 'person_emp_length', 'loan_int_rate',\n",
    "                    'loan_amnt', 'loan_percent_income', 'cb_person_default_on_file',  \n",
    "                    'cb_person_cred_hist_length','person_home_ownership_MORTGAGE', \n",
    "                    'person_home_ownership_OTHER', 'person_home_ownership_OWN', \n",
    "                    'person_home_ownership_RENT', 'loan_intent_DEBTCONSOLIDATION',\n",
    "                    'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL',\n",
    "                    'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_grade_A', 'loan_grade_B',\n",
    "                    'loan_grade_C', 'loan_grade_D','loan_grade_E', 'loan_grade_F', 'loan_grade_G'\n",
    "                    ]\n",
    "\n",
    "\n",
    "# Step 1: Initialize and train the model\n",
    "rf_model = RandomForestModel(\n",
    "    df=encoded_df,\n",
    "    features=original_features,\n",
    "    target=target,\n",
    "    test_size=0.2,\n",
    "    eval_size=0.0, \n",
    "    random_state=42,\n",
    "    scoring='roc_auc',\n",
    "    balance_method='none',  \n",
    "    tune_hyperparameters=False,\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    criterion='entropy'\n",
    ")\n",
    "\n",
    "# Step 2: Train the model\n",
    "rf_model.fit_model()\n",
    "\n",
    "# Step 3: Get Gini Importance\n",
    "gini_df = rf_model.get_feature_importance_gini()\n",
    "print(\"🔹 Gini Importance:\\n\", gini_df)\n",
    "\n",
    "\n",
    "# # Step 4: Get Permutation Importance\n",
    "# permutation_df = rf_model.get_permutation_importance(dataset='test', scoring='roc_auc')\n",
    "# print(\"🔹 Permutation Importance:\\n\", permutation_df)\n",
    "\n",
    "# # Step 5: Get SHAP Importance\n",
    "# shap_df = rf_model.get_feature_importance_shap(max_display=30)\n",
    "# print(\"🔹 SHAP Importance:\\n\", shap_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac62e77f",
   "metadata": {},
   "source": [
    "#### 5.1.3 Calculate Feature Importance with XGBoost\n",
    "XGBoost (Extreme Gradient Boosting) is a high-performance gradient boosting framework known for its accuracy, speed, and regularization capabilities, making it ideal for structured/tabular data.\n",
    "\n",
    "In this step:\n",
    "\n",
    "- We train an XGBoost classifier on the preprocessed dataset (with original numerical features and one-hot encoded categorical variables, without WoE transformation).\n",
    "\n",
    "- We evaluate feature importance using three complementary methods:\n",
    "\n",
    "    - Gain Importance:\n",
    "    Gain measures the average improvement in the loss function brought by a feature when it is used in a split across all boosting rounds. Features with higher gain are considered more important. Unlike Gini importance, gain is based on how much each feature contributes to reducing the prediction error.\n",
    "\n",
    "    - Permutation Importance\n",
    "\n",
    "    - SHAP Values (SHapley Additive exPlanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb3750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize and train the model\n",
    "xgb_model = XGBoostModel(\n",
    "    df=encoded_df,\n",
    "    features=original_features,\n",
    "    target=target,\n",
    "    test_size=0.2,\n",
    "    eval_size=0.0, \n",
    "    random_state=42,\n",
    "    scoring='roc_auc',\n",
    "    balance_method='none',\n",
    "    tune_hyperparameters=False,\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "# Step 2: Train the model\n",
    "xgb_model.fit_model()\n",
    "\n",
    "# Step 3: Get Gain Importance\n",
    "gain_df = xgb_model.get_feature_importance_gain()\n",
    "print(\"🔹 Gain Importance:\\n\", gain_df)\n",
    "\n",
    "# Step 4: Get Permutation Importance\n",
    "xgb_permutation_df = xgb_model.get_permutation_importance(dataset='test', scoring='roc_auc')\n",
    "print(\"🔹 Permutation Importance:\\n\", xgb_permutation_df)\n",
    "\n",
    "# Step 5: Get SHAP Importance\n",
    "xgb_shap_df = xgb_model.get_feature_importance_shap(max_display=30)\n",
    "print(\"🔹 SHAP Importance:\\n\", xgb_shap_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
