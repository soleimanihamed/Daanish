{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.simulation.sim_utils import RandomSimulator\n",
    "from utils.core.save_manager import SaveUtils\n",
    "from utils.data_io import load_data\n",
    "import os\n",
    "from utils.viz.general_viz import Visualisation\n",
    "from utils.eda.correlation import CorrelationAnalyzer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.simulation.decomposition import Decomposer\n",
    "from utils.simulation.covariance_matrix import CovarianceMatrix\n",
    "from utils.simulation.monte_carlo_simulator import MonteCarloSimulator\n",
    "from utils.simulation.transformation import DistributionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "üß™ Simulating Unscaled Normally Distributed Data with Specified Skewness and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_column_names = [\n",
    "    \"Interest_Rate\", \"Loan_Amount\", \"Employment_Length\",\n",
    "    \"Age\", \"Income\", \"Credit_Score\",\n",
    "    \"Debt_To_Income\", \"Home_Ownership\", \"Purpose\", \"Region\"\n",
    "]\n",
    "\n",
    "simulator = RandomSimulator(num_simulations=1000,column_names=custom_column_names)\n",
    "df = simulator.simulate_normal(num_variables=10, target_skew=0, target_kurt=3)\n",
    "\n",
    "save_util = SaveUtils()\n",
    "save_util.save_dataframe_to_csv(df,os.path.join(os.getcwd(), \"data/output/simulated_normal.csv\"), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "üß™ Simulate scaled normals (e.g., volatilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [0.2, 0.15, 0.3]  # std devs or volatilities\n",
    "simulator = RandomSimulator(parameters=params)\n",
    "df = simulator.simulate_normal(target_skew=0, target_kurt=3)\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "üìä Simulating Poisson Data from Excel Input\n",
    "- Reads input data from an Excel file\n",
    "- Initializes a Poisson simulator\n",
    "- Simulates Poisson-distributed values\n",
    "- Saves the simulated data to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input  = load_data(\n",
    "    source_type='excel',\n",
    "    input_path= os.path.join(os.getcwd(), \"data/input/Poisson Simulation.xlsx\"),\n",
    "    sheet_name='Lambda Calculation',\n",
    "    usecols=['Class','Lambda']\n",
    ")\n",
    "parameters = df_input[\"Lambda\"].values\n",
    "column_names = [f\"Class_{int(c)}\" for c in df_input[\"Class\"].values]\n",
    "\n",
    "simulator = RandomSimulator(parameters=parameters, column_names=column_names,num_simulations = 10000)\n",
    "sim = simulator.simulate_poisson()\n",
    "df_poisson = sim.round().astype(int)\n",
    "save_util.save_dataframe_to_excel(\n",
    "    df_poisson,\n",
    "    os.path.join(os.getcwd(), \"data/output/simulated_poisson.xlsx\"),\n",
    "    sheet_name=\"simulated values\", \n",
    "    overwrite=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### üîó 2.5 Examine Variable Correlations\n",
    "\n",
    "This section calculates and displays correlations between different types of variables in the `main_df`.\n",
    "\n",
    "- **`num_method` (str)**: Defines the method for calculating correlation between numerical variables. Allowed values are:\n",
    "    - `'pearson'` *(default)*: Standard Pearson linear correlation coefficient.\n",
    "    - `'spearman'`: Spearman's rank correlation coefficient (for monotonic relationships).\n",
    "    - `'kendall'`: Kendall's tau correlation coefficient (for ordinal or non-normally distributed data).\n",
    "\n",
    "- **`cat_method` (str)**: Defines the method for calculating association between categorical variables. Allowed values are:\n",
    "    - `'cramers_v'` *(default)*: Cramer's V (measures association between nominal categorical variables).\n",
    "    - `'mutual_info'`: Mutual Information (measures the statistical dependence between two random variables).\n",
    "\n",
    "- **`cat_num_method` (str)**: Defines the method for calculating association between categorical and numerical variables. Allowed values are:\n",
    "    - `'correlation_ratio'` *(default)*: Correlation Ratio (Eta squared, measures variance explained).\n",
    "    - `'f_test'`: F-statistic from ANOVA (assesses the difference in means across categories).\n",
    "    - `'mutual_info'`: Mutual Information (measures the statistical dependence). \n",
    "    - `'kruskal'`: Non-parametric alternative to ANOVA. Compares distributions of a continuous variable across categories. Good when your numerical variables are not normally distributed\n",
    "    - `'target_spearman'`: Replaces each category with the mean of the target variable (e.g. default rate). Then computes correlation with numerical features. Captures ordinal structure or monotonic trends across groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = load_data(\n",
    "    source_type='csv',\n",
    "    input_path= os.path.join(os.getcwd(), \"data/input/returns_raw.csv\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = CorrelationAnalyzer(main_df)\n",
    "corr_df,corr_matrix = analyzer.correlation_matrix(num_method=\"pearson\", cat_method=\"cramers_v\",\n",
    "                                      cat_num_method=\"correlation_ratio\",return_matrix=True)\n",
    "\n",
    "Visualisation.plot_heatmap_matrix(corr_matrix, title=\"Correlation Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "CovarianceMatrix\n",
    "----------------\n",
    "A utility class for calculating the covariance matrix of asset returns,\n",
    "optionally annualized. Intended for use in financial simulations such as\n",
    "Monte Carlo modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_calc = CovarianceMatrix(main_df)\n",
    "cov_matrix = cov_calc.get_matrix()\n",
    "# print(cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Cholesky Decomposition and Visualisation\n",
    "\n",
    "We perform a Cholesky decomposition on the correlation matrix to obtain a lower triangular matrix.  \n",
    "This decomposition is useful for simulations and generating correlated random variables.  \n",
    "We then visualise the resulting matrix using a heatmap for better interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "Decomposition_df = Decomposer.cholesky_decomposition(cov_matrix)\n",
    "# Visualisation.plot_heatmap_matrix(Decomposition_df, title=\"Cholesky Decomposition Matrix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "##### üìä Monte Carlo Simulation Using Rubinstein's Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_df = load_data(\n",
    "#     source_type='csv',\n",
    "#     input_path= os.path.join(os.getcwd(), \"data/input/Monte Carlo Multivariable.csv\"),\n",
    "# )\n",
    "main_df = load_data(\n",
    "    source_type='csv',\n",
    "    input_path= os.path.join(os.getcwd(), \"data/input/Monte Carlo Univariable.csv\"),\n",
    ")\n",
    "\n",
    "sim = MonteCarloSimulator(main_df,num_simulations=10000)\n",
    "sim.run_simulation()\n",
    "multivariate_MC_simulation = sim.get_final_simulated_values()\n",
    "# covariance_matrix = sim.get_covariance_matrix()\n",
    "raw_normal_simulation = sim.get_raw_simulations()\n",
    "# cholesky_matrix = sim.get_cholesky_matrix()\n",
    "\n",
    "save_util.save_dataframe_to_csv(\n",
    "    multivariate_MC_simulation,\n",
    "    os.path.join(os.getcwd(), \"data/output/MonteCarlo final_sim_u.csv\"),\n",
    "    overwrite=True\n",
    "    )\n",
    "\n",
    "save_util.save_dataframe_to_csv(\n",
    "    raw_normal_simulation,\n",
    "    os.path.join(os.getcwd(), \"data/output/MonteCarlo random_sim_u.csv\"),\n",
    "    overwrite=True\n",
    "    )\n",
    "\n",
    "# save_util.save_dataframe_to_csv(\n",
    "#     covariance_matrix,\n",
    "#     os.path.join(os.getcwd(), \"data/output/MonteCarlo cov_matrix.csv\"),\n",
    "#     overwrite=True\n",
    "#     )\n",
    "\n",
    "\n",
    "# save_util.save_dataframe_to_csv(\n",
    "#     cholesky_matrix,\n",
    "#     os.path.join(os.getcwd(), \"data/output/MonteCarlo cholesky_matrix.csv\"),\n",
    "#     overwrite=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Reading From a Json Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_io import load_data\n",
    "api_data = '[[\"name\", \"age\"], [\"Alice\", 30], [\"Bob\", 25]]'\n",
    "df = load_data(source_type='json', json_source=api_data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Fitting **Beta** distributions and then simulating random numbers based on the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eda.statistical import StatisticalAnalysis\n",
    "from utils.simulation.sim_utils import RandomSimulator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Step 1: Create sample dataframe with random numbers between 0 and 1\n",
    "np.random.seed(42)  # for reproducibility\n",
    "df = pd.DataFrame({\n",
    "    'default_rate': np.random.rand(100)\n",
    "})\n",
    "\n",
    "\n",
    "# Step 2: Initialize StatisticalAnalysis\n",
    "stat = StatisticalAnalysis(df)\n",
    "\n",
    "# Step 3: Fit only 'beta' and 'logit-normal'\n",
    "distribution_results = stat.fit_best_distribution(\n",
    "    ['default_rate'],\n",
    "    method='sumsquare_error',\n",
    "    common_distributions=False,\n",
    "    distribution_list=['beta'],  # assuming 'logitnorm' works with Fitter\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "print(\"\\nReturned Results:\", distribution_results)\n",
    "\n",
    "\n",
    "params = distribution_results['default_rate']['parameters']\n",
    "simulator = RandomSimulator(\n",
    "    parameters=[(params['a'], params['b'], params['loc'], params['scale'])],\n",
    "    num_simulations=10000,\n",
    "    column_names=['default_rate_sim']\n",
    ")\n",
    "df_sim = simulator.simulate_beta()\n",
    "print(df_sim.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "A sample for LDA report \n",
    "How to use log-normal and gamma simulations in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eda.statistical import StatisticalAnalysis\n",
    "from utils.simulation.sim_utils import RandomSimulator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Create sample dataframe\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'default_rate': np.random.randint(100, 1001, size=100)  # 100 integers from 100 to 1000 inclusive\n",
    "})\n",
    "\n",
    "# Step 2: Initialize StatisticalAnalysis\n",
    "stat = StatisticalAnalysis(df)\n",
    "\n",
    "# Step 3: Fit only 'logit-normal' and 'gamma'\n",
    "distribution_results = stat.fit_best_distribution(\n",
    "    ['default_rate'],\n",
    "    method='sumsquare_error',\n",
    "    common_distributions=False,\n",
    "    distribution_list=['lognorm', 'gamma'],\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "print(\"\\nReturned Results:\", distribution_results)\n",
    "\n",
    "# Step 4: Loop through results and simulate using the winning distribution\n",
    "for col, res in distribution_results.items():\n",
    "    best_dist = res['best_distribution']   # e.g., 'lognorm' or 'gamma'\n",
    "    params = res['parameters']\n",
    "\n",
    "    simulator = RandomSimulator(\n",
    "        parameters=[tuple(params.values())],  # convert dict to tuple\n",
    "        num_simulations=10000,\n",
    "        column_names=[f\"{col}_sim\"]\n",
    "    )\n",
    "\n",
    "    if best_dist == 'lognorm':\n",
    "        ordered_params = (params['s'], params['loc'], params['scale'])\n",
    "        simulator = RandomSimulator(\n",
    "            parameters=[ordered_params],\n",
    "            num_simulations=10000,\n",
    "            column_names=[f\"{col}_sim\"]\n",
    "        )\n",
    "        df_sim = simulator.simulate_lognormal()\n",
    "    elif best_dist == 'gamma':\n",
    "        ordered_params = (params['a'], params['loc'], params['scale'])\n",
    "        simulator = RandomSimulator(\n",
    "            parameters=[ordered_params],\n",
    "            num_simulations=10000,\n",
    "            column_names=[f\"{col}_sim\"]\n",
    "        )\n",
    "        df_sim = simulator.simulate_gamma()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distribution: {best_dist}\")\n",
    "\n",
    "    print(f\"\\nSimulation for {col} ({best_dist}):\")\n",
    "    print(df_sim.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Gaussian Copula with Monte Carlo Simulation Sample\n",
    "- 1.simulate random initial values with given distributions\n",
    "- 2.fit best given distributions on them\n",
    "- 3.calculate CDF to map them between (0,1)\n",
    "- 4.convert them to standard normal\n",
    "- 5.calculate correlation\n",
    "- 6.calculate cholesky\n",
    "- 7.Simulating Random numbers from standard normal distribution\n",
    "- 8.Multiplying simulated data by Cholesky matrix\n",
    "- 9.Calculating Normal CDF\n",
    "- 10.converting the result to their real distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "##### 1.simulate random initial values with given distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random numbers\n",
    "\n",
    "variables = {\n",
    "    \"A1\": {\"dist\": \"lognormal\", \"mean\": 1500000},\n",
    "    \"A2\": {\"dist\": \"lognormal\", \"mean\": 900000},\n",
    "    \"A3\": {\"dist\": \"normal\", \"mean\": 600000},\n",
    "    \"A4\": {\"dist\": \"normal\", \"mean\": 3000000},\n",
    "    \"A5\": {\"dist\": \"gamma\", \"mean\": 5000000},\n",
    "    \"A6\": {\"dist\": \"gamma\", \"mean\": 200000},\n",
    "    \"A7\": {\"dist\": \"lognormal\", \"mean\": 300000},\n",
    "    \"A8\": {\"dist\": \"gamma\", \"mean\": 1700000},\n",
    "    \"A9\": {\"dist\": \"normal\", \"mean\": 2800000},\n",
    "    \"A10\": {\"dist\": \"lognormal\", \"mean\": 500000},\n",
    "}\n",
    "\n",
    "def build_parameters(dist, mean):\n",
    "    if dist == \"normal\":\n",
    "        std = 0.2 * mean\n",
    "        return np.array([[mean, std]])  # (mean, std)\n",
    "    elif dist == \"lognormal\":\n",
    "        s = 0.4\n",
    "        loc = 0\n",
    "        scale = mean / 1.2\n",
    "        return np.array([[s, loc, scale]])\n",
    "    elif dist == \"gamma\":\n",
    "        shape = 2\n",
    "        loc = 0\n",
    "        scale = mean / shape\n",
    "        return np.array([[shape, loc, scale]])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distribution type: {dist}\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "for var_name, specs in variables.items():\n",
    "    dist = specs[\"dist\"]\n",
    "    mean = specs[\"mean\"]\n",
    "\n",
    "    params = build_parameters(dist, mean)\n",
    "\n",
    "    sim = RandomSimulator(parameters=params, num_simulations=186, column_names=[var_name], decorrelate=False)\n",
    "\n",
    "    if dist == \"normal\":\n",
    "        df = sim.simulate_normal()\n",
    "    elif dist == \"lognormal\":\n",
    "        df = sim.simulate_lognormal()\n",
    "    elif dist == \"gamma\":\n",
    "        df = sim.simulate_gamma()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distribution type: {dist}\")\n",
    "\n",
    "    results[var_name] = df[var_name]\n",
    "\n",
    "final_df = pd.concat(results, axis=1)\n",
    "\n",
    "print(final_df.head())\n",
    "\n",
    "save_util.save_dataframe_to_excel(\n",
    "    final_df,\n",
    "    os.path.join(os.getcwd(), \"data/output/LAR_simulated_Default.xlsx\"),\n",
    "    sheet_name=\"simulated values\", \n",
    "    overwrite=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "##### 2.fit best given distributions\n",
    "We only use these distributions: 'norm, 'lognorm' , 'gamma', 'expon', 'weibull_min', 'beta','t','pareto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read data\n",
    "main_df = load_data(\n",
    "    source_type='excel',\n",
    "    input_path= os.path.join(os.getcwd(), \"data/input/LAR_simulated_Default.xlsx\"),\n",
    "    sheet_name = 'historical values'\n",
    ")\n",
    "\n",
    "# Step 2: Read features\n",
    "features = main_df.columns.to_list()\n",
    "\n",
    "# Step 3: Initialize StatisticalAnalysis\n",
    "stat = StatisticalAnalysis(main_df)\n",
    "\n",
    "# Step 4: Fit only given distributions\n",
    "distribution_results = stat.fit_best_distribution(\n",
    "    features,\n",
    "    method='sumsquare_error',\n",
    "    common_distributions=False,\n",
    "    distribution_list=['norm', 'lognorm' , 'gamma', 'expon', 'weibull_min', 'beta','t','pareto'],\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "print(\"\\nReturned Results:\", distribution_results)\n",
    "\n",
    "# Step 5: Format and save the distribution results in a file\n",
    "formatted = {}\n",
    "\n",
    "for var, info in distribution_results.items():\n",
    "    params = info['parameters']\n",
    "    formatted[var] = {'distribution': info['best_distribution'], **params}\n",
    "\n",
    "distribution_df = pd.DataFrame(formatted)\n",
    "\n",
    "# Print neatly\n",
    "print(distribution_df)\n",
    "\n",
    "save_util.save_dataframe_to_excel(\n",
    "    distribution_df,\n",
    "    os.path.join(os.getcwd(), \"data/input/LAR_simulated_Default.xlsx\"),\n",
    "    sheet_name=\"Features Distributions\", \n",
    "    overwrite=False,\n",
    "    index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "##### 3.calculate CDF to map them between (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = StatisticalAnalysis(main_df)\n",
    "cdf_df = stat.compute_cdf(distribution_results)\n",
    "\n",
    "print(cdf_df.head())\n",
    "\n",
    "save_util.save_dataframe_to_excel(\n",
    "    cdf_df,\n",
    "    os.path.join(os.getcwd(), \"data/input/LAR_simulated_Default.xlsx\"),\n",
    "    sheet_name=\"cdf\", \n",
    "    overwrite=False,\n",
    "    index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "##### 4.convert CDF to Standard Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.simulation.transformation import DistributionTransformer\n",
    "\n",
    "# Convert CDFs to standard normal space\n",
    "z_df = DistributionTransformer.to_standard_normal(cdf_df)\n",
    "\n",
    "save_util.save_dataframe_to_excel(\n",
    "    z_df,\n",
    "    os.path.join(os.getcwd(), \"data/input/LAR_simulated_Default.xlsx\"),\n",
    "    sheet_name=\"Z normal\", \n",
    "    overwrite=False,\n",
    "    index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "##### 5.calculate correlation\n",
    "Using Correlation Matrix with Pearson method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = CorrelationAnalyzer(z_df)\n",
    "z_corr_df,z_corr_matrix = analyzer.correlation_matrix(num_method=\"pearson\",return_matrix=True)\n",
    "\n",
    "Visualisation.plot_heatmap_matrix(z_corr_matrix, title=\"Correlation Matrix\")\n",
    "\n",
    "z_correlation_pivot = analyzer.show_correlation_pivot(num_method=\"pearson\")\n",
    "print(z_correlation_pivot)\n",
    "\n",
    "save_util.save_dataframe_to_excel(\n",
    "    z_correlation_pivot,\n",
    "    os.path.join(os.getcwd(), \"data/input/LAR_simulated_Default.xlsx\"),\n",
    "    sheet_name=\"Correlation\", \n",
    "    overwrite=False,\n",
    "    index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "##### 6.Calculate Cholesky Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_Cholesky_df = Decomposer.cholesky_decomposition(z_correlation_pivot)\n",
    "\n",
    "print(z_Cholesky_df)\n",
    "\n",
    "save_util.save_dataframe_to_excel(\n",
    "    z_Cholesky_df,\n",
    "    os.path.join(os.getcwd(), \"data/input/LAR_simulated_Default.xlsx\"),\n",
    "    sheet_name=\"Cholesky\", \n",
    "    overwrite=False,\n",
    "    index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "##### 7.Simulating Random numbers from standard normal distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract feature names from main_df\n",
    "feature_names = main_df.columns.tolist()\n",
    "\n",
    "# Step 2: Initialize the RandomSimulator class\n",
    "RandomNormalSimulator = RandomSimulator(\n",
    "    num_simulations=10000,\n",
    "    column_names=feature_names,\n",
    "    decorrelate=True\n",
    ")\n",
    "\n",
    "# Step 3: Generate uncorrelated normal distributions for all features\n",
    "raw_simulated_data = RandomNormalSimulator.simulate_normal(\n",
    "    num_variables=len(feature_names),\n",
    "    loc=0,\n",
    "    scale=1,\n",
    "    target_skew=0,\n",
    "    target_kurt=3\n",
    ")\n",
    "\n",
    "save_util.save_dataframe_to_excel(\n",
    "    raw_simulated_data,\n",
    "    os.path.join(os.getcwd(), \"data/input/LAR_simulated_Default.xlsx\"),\n",
    "    sheet_name=\"raw_simulations\", \n",
    "    overwrite=False,\n",
    "    index=True\n",
    "    )\n",
    "\n",
    "print(raw_simulated_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "##### 8.Multiplying simulated data by Cholesky matrix to create correlation between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure column order alignment\n",
    "cholesky_matrix = z_Cholesky_df.loc[main_df.columns, main_df.columns]  # reorder to match features\n",
    "simulated_data = raw_simulated_data[main_df.columns]  # align simulated columns\n",
    "\n",
    "# Step 2: Matrix multiplication (introduce correlations)\n",
    "correlated_data = simulated_data.to_numpy() @ cholesky_matrix.to_numpy().T\n",
    "\n",
    "# Step 3: Convert back to DataFrame with feature names\n",
    "correlated_df = pd.DataFrame(correlated_data, columns=main_df.columns)\n",
    "\n",
    "# Step 4: Display first few rows\n",
    "print(correlated_df.head())\n",
    "\n",
    "save_util.save_dataframe_to_excel(\n",
    "    correlated_df,\n",
    "    os.path.join(os.getcwd(), \"data/input/LAR_simulated_Default.xlsx\"),\n",
    "    sheet_name=\"Correlated_simulations\", \n",
    "    overwrite=False,\n",
    "    index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "##### 9.Calculating Normal CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Since our simulated variables are already perfectly normal (from Cholesky-transformed standard normals),\n",
    "#         there is no need to re-fit any distributions. We can safely assume each feature follows N(0, 1).\n",
    "\n",
    "fitted_results = {\n",
    "    feature: {\n",
    "        \"best_distribution\": \"norm\",\n",
    "        \"parameters\": {\"loc\": 0, \"scale\": 1}\n",
    "    }\n",
    "    for feature in correlated_df.columns\n",
    "}\n",
    "\n",
    "# Step 2: Compute the cumulative distribution function (CDF) values\n",
    "#         for each feature based on the standard normal distribution.\n",
    "\n",
    "cdf_simulated_df = StatisticalAnalysis(correlated_df).compute_cdf(fitted_results)\n",
    "\n",
    "# Step 3: Display a sample of the computed CDF values\n",
    "print(cdf_simulated_df.head())\n",
    "\n",
    "save_util.save_dataframe_to_excel(\n",
    "    cdf_simulated_df,\n",
    "    os.path.join(os.getcwd(), \"data/input/LAR_simulated_Default.xlsx\"),\n",
    "    sheet_name=\"cdf_simulated_data\", \n",
    "    overwrite=False,\n",
    "    index=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "##### 10.converting the result to their real distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert 'distribution_results' to match the expected input format of from_standard_normal_to_real()\n",
    "fitted_params = {\n",
    "    feature: {\n",
    "        \"distribution\": info[\"best_distribution\"],\n",
    "        **info[\"parameters\"]\n",
    "    }\n",
    "    for feature, info in distribution_results.items()\n",
    "}\n",
    "\n",
    "# Step 2: Convert correlated standard normal data back to real-world scale\n",
    "\n",
    "real_values_df = DistributionTransformer.from_standard_normal_to_real(\n",
    "    z_df=cdf_simulated_df,        # The correlated data in CDF/Œ¶‚Åª¬π space\n",
    "    fitted_params=fitted_params   # Original fitted distribution parameters\n",
    ")\n",
    "\n",
    "# Step 3: Display result ---\n",
    "print(real_values_df.head())\n",
    "\n",
    "\n",
    "save_util.save_dataframe_to_excel(\n",
    "    real_values_df,\n",
    "    os.path.join(os.getcwd(), \"data/input/LAR_simulated_Default.xlsx\"),\n",
    "    sheet_name=\"real_values\", \n",
    "    overwrite=False,\n",
    "    index=True\n",
    "    )  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
